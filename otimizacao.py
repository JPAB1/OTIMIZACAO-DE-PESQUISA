# -*- coding: utf-8 -*-
"""otimizacao.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ohNb8KCK_vy7-UPZFqq8qcHUqRpR_6GB

Instalação do PySpark
"""

! pip install pyspark

"""Importações dos pacotes"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from py4j.java_gateway import select

"""Criando a ceção do spark"""

spark = SparkSession.builder.getOrCreate()

"""1 - Lendo o arquivo 'videos-preparados.snappy.parquet' no dataframe 'df_video' e o arquivo ‘video-comments-tratados.snappy.parquet' no dataframe 'df_comments'"""

df_video = spark.read.option("header", "true").parquet('drive/MyDrive/Colab Notebooks/spark/data/material_de_apoio_m27/videos-preparados.snappy.parquet')
df_comments = spark.read.option("header", "true").parquet('drive/MyDrive/Colab Notebooks/spark/data/material_de_apoio_m27/videos-comments-tratados.snappy.parquet')

"""2 - Criando tabelas temporárias para ambos os dataframe"""

df_video.createOrReplaceTempView("df_video")
df_comments.createOrReplaceTempView("df_comments")

"""3- Fazendo um join das tabelas criadas anteriormente utilizando o spark.sql no dataframe ‘join_video_comments’"""

join_video_comments = spark.sql("""
SELECT v.Title, v.`Video ID`, c.Comments, c.Likes
FROM df_video v
JOIN df_comments c ON v.`Video ID` = c.`Video ID`
""")

join_video_comments.show()

"""4- Fazendo as mesmas etapas anteriores (1,2,3) utilizando repartition e coalesce"""

# Lendo os arquivos já com uma redistribuição que desejei das partições (repartition())
df_video_rep = spark.read.option("header", "true").parquet('drive/MyDrive/Colab Notebooks/spark/data/material_de_apoio_m27/videos-preparados.snappy.parquet').repartition(3)
df_comments_rep = spark.read.option("header", "true").parquet('drive/MyDrive/Colab Notebooks/spark/data/material_de_apoio_m27/videos-comments-tratados.snappy.parquet').repartition(3)

print('Número de partições com  repartition de df_video_rep: ', df_video_rep.rdd.getNumPartitions())
print('Número de partições com  repartition de df_comments_rep: ', df_comments_rep.rdd.getNumPartitions())

# Criando tabelas temporárias
df_video.createOrReplaceTempView("df_video")
df_comments.createOrReplaceTempView("df_comments")

# Reduzindo a quantidade de partições. Essa função só permite diminuir a quantidade das partições
df_video_rdd = df_video.coalesce(2)
df_comments_rdd = df_comments.coalesce(2)

print('Número de partições após coalesce en df_video: ', df_video_rdd.rdd.getNumPartitions())
print('Número de partições após coalesce en df_comments: ', df_comments_rdd.rdd.getNumPartitions())

# Fazendo uma consulta usando repartition para criar mais partições
join_video_comments = spark.sql("""
SELECT v.Title, v.`Video ID`, c.Comments, c.Likes
FROM df_video v
JOIN df_comments c ON v.`Video ID` = c.`Video ID`
""").repartition(3)

print('Número de partições com repartition de  join_video_comments: ', join_video_comments.rdd.getNumPartitions())

# Redimensionando as partições
join_video_comments_rdd =join_video_comments.coalesce(1)

print('Número de partições com coalesce de  join_video_comments: ', join_video_comments_rdd.rdd.getNumPartitions())

"""5- Utilizando o explain para entender melhor as duas formas de realizar as etapas e refaça novamente as etapas anteriores (1,2,3,), utilizando tudo que você já aprendeu para realizar o join e filter apenas com os dados necessários."""

# O explain nos dataframe lendo os Arquivos de forma (normal)
print('Aqruivos sem partições alteradas manualmente:n/')
df_video.explain()
df_comments.explain()

# O explain nos datafrema com os arquivos particionados manualmente

print('Arquivos com partições alteradas manualmente:n/')
df_video_rdd.explain()
df_comments_rdd.explain()



# Usando o método de filtrar para uma consulta otimizada no join
media_likes = df_video.select(avg("Likes").alias('media_likes'))
join_video_comments_otimizado = df_video.join(
    media_likes,
    df_video['Likes'] > media_likes['media_likes']
).select(df_video['*'])

media_likes.show()
join_video_comments_otimizado.show()

"""6 - Salvando o seu join otimizado como 'join-videos-comments-otimizado' no formato parquet"""

join_video_comments_otimizado.write.mode("overwrite").parquet('drive/MyDrive/Colab Notebooks/spark/data/join-videos-comments-otimizado_parquet')

spark.stop()